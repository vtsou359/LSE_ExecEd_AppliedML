{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hphZB6I2s5OZ"
   },
   "source": [
    "# LSE Machine Learning: Practical Applications\n",
    "## Module 4 Unit 2 IDE Activity (Practice) | Practical application of shrinkage methods in R\n",
    "In this IDE activity, you have the opportunity to practise executing shrinkage methods on a data set in R. You will specifically learn about ridge regression and the lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHZe9EJTtbr6"
   },
   "source": [
    "### Step 1: Load the relevant packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcL9jhYssEYL"
   },
   "source": [
    "To execute ridge regression on the credit card data set, the tidyverse, glmnet, caret, ISLR, and psych packages are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6uTRi1A1sEYO"
   },
   "outputs": [],
   "source": [
    "# Set the seed and load packages required\n",
    "set.seed(123)\n",
    "library(tidyverse)\n",
    "library(glmnet)\n",
    "library(caret)     \n",
    "library(ISLR)\n",
    "library(psych)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkKhxmwWt5kx"
   },
   "source": [
    "### Step 2: Load and prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgIVDolXuC0h"
   },
   "source": [
    "This example uses the credit card data set. The data is loaded from the ISLR package.\n",
    "Once the data has been loaded, it must be manipulated, as the glmnet package requires separate predictor and predicted variables. For this data manipulation process, the predicted variable, *y*, needs to be separated from the predictor variables, *x*. In this example, _**balance**_ is the variable that must be predicted. To separate the variables, use the `select` function.\n",
    "\n",
    "Once the variables are separated, dummy variables need to be created. In this case, print the predictor variables in a matrix format for the glmnet package. Use the `as.matrix` function for the predicted variable and the `model.matrix` function for the predictor variables.The `model.matrix` function also creates the dummy variables that were created manually in the previous unit.\n",
    "\n",
    "With the data prepared, fit the models onto the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZuwtpHLsEYS"
   },
   "outputs": [],
   "source": [
    "# Load and prepare the data\n",
    "Credit <- Credit\n",
    "Credit <- select(Credit,-c(ID)) # Remove the ID\n",
    "Credit <- na.omit(Credit)         # Make sure that there are no NA values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VTwO2K9yPrB"
   },
   "source": [
    "Split the data into separate predicted and predictor values. Ensure that the categorical variables are transformed using the `model.matrix` function, and scale the predicted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zrVmJDYvPmu"
   },
   "outputs": [],
   "source": [
    "y <- Credit %>% select(Balance) %>% scale(center = TRUE, scale = FALSE) %>% as.matrix()\n",
    "x <- model.matrix( ~ . - 1, select(Credit,-c(Balance)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZkbKZs3wAEf"
   },
   "source": [
    "### Step 3: Execute shrinkage methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNpaF_5ovPmx"
   },
   "source": [
    "#### Ridge regression method\n",
    "Apply the ```cv.glmnet``` function to estimate the performance of the model across various values of the shrinkage estimator, lambda. This package fits generalised linear models via penalised maximum likelihood, and can be used for a number of different models. In this case, it is used for both ridge regression and the lasso, starting with the ridge regression example.\n",
    "\n",
    "Add the *x* and *y* parameters, and set the family to Gaussian, as it is a regression problem with Gaussian errors (specifically, *y* is Gaussian). Note that the Gaussian family is the default, so this parameter is omitted. To fit a ridge regression, alpha is set to 0, and to fit a lasso model, alpha is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYU_yR-4vPmz"
   },
   "outputs": [],
   "source": [
    "# Execute 10-fold cross-validation to select the optimal lambda\n",
    "lambdas_to_try <- 10^seq(0, 10, length.out = 100)\n",
    "\n",
    "# Set alpha to 0 to implement ridge regression\n",
    "ridge_cv <- cv.glmnet(x, y, alpha = 0, lambda = lambdas_to_try,\n",
    "                      standardize = TRUE, nfolds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wq_LOPfjvPm2",
    "outputId": "57df6a03-ac54-48a8-826d-10057f333f10"
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plot(ridge_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfFNbhq-vPm6"
   },
   "outputs": [],
   "source": [
    "# Determine the best cross-validated lambda for the ridge regression example\n",
    "lambda_cv <- ridge_cv$lambda.min\n",
    "\n",
    "# Fit the final model, and calculate RSS and multiple R-squared values\n",
    "model_cv <- glmnet(x, y, alpha = 0, lambda = lambda_cv, standardize = TRUE)\n",
    "y_hat_cv <- predict(model_cv, x)\n",
    "ssr_cv <- t(y - y_hat_cv) %*% (y - y_hat_cv)\n",
    "rsq_ridge_cv <- cor(y, y_hat_cv)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VCMLOPSvPnA",
    "outputId": "18c349b5-c14a-4d08-bf42-efba59142d58"
   },
   "outputs": [],
   "source": [
    "# Determine how the coefficients are shrunk by the increasing lambda\n",
    "res <- glmnet(x, y, alpha = 0, lambda = lambdas_to_try, standardize = FALSE)\n",
    "plot(res, xvar = \"lambda\")\n",
    "legend(\"bottomright\", lwd = 1, col = 1:6, legend = colnames(x), cex = .7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm6-HKPZxYCn"
   },
   "source": [
    "Notice how the increasing lambda shrinks the coefficients towards 0. The lines in the graph represent the coefficients per variable for the different lambdas. As the lambda increases, coefficients are shrunk towards 0, but never to 0 exactly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd3LWr89vPnD"
   },
   "source": [
    "#### The lasso method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3nsKPo2zDfJ"
   },
   "source": [
    "The lasso method is executed similarly to ridge regression. In the next code cell, 10-fold cross-validation is executed to select the optimal lambda, followed by the application of the lasso method, before plotting the cross-validation results. Note that alpha is set to 1 in this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0nlEqaNvPnE",
    "outputId": "4e8fe3f2-d28f-41bc-893b-5c3c7d49192b"
   },
   "outputs": [],
   "source": [
    "# Execute 10-fold cross-validation to select the optimal lambda\n",
    "lambdas_to_try <- 10^seq(0, 10, length.out = 100)\n",
    "\n",
    "# Perform the lasso method by setting alpha = 1\n",
    "lasso_cv <- cv.glmnet(x, y, alpha = 1, lambda = lambdas_to_try,\n",
    "                      standardize = TRUE, nfolds = 10)\n",
    "# Plot the results\n",
    "plot(lasso_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUXShxamvPnH",
    "outputId": "3df1d111-3900-41a1-d966-742d4c3866c9"
   },
   "outputs": [],
   "source": [
    "# Determine the best cross-validated lambda for the lasso example\n",
    "lambda_cv <- lasso_cv$lambda.min\n",
    "\n",
    "# Fit the final model, and calculate RSS and multiple R-squared values\n",
    "model_cv2 <- glmnet(x, y, alpha = 1, lambda = lambda_cv, standardize = TRUE)\n",
    "y_hat_cv2 <- predict(model_cv2, x)\n",
    "ssr_cv2 <- t(y - y_hat_cv2) %*% (y - y_hat_cv2)\n",
    "rsq_lasso_cv2 <- cor(y, y_hat_cv2)^2\n",
    "\n",
    "# Determine how the coefficients are shrunk by the increasing lambda\n",
    "res2 <- glmnet(x, y, alpha = 1, lambda = lambdas_to_try, standardize = FALSE)\n",
    "plot(res2, xvar = \"lambda\")\n",
    "legend(\"bottomright\", lwd = 1, col = 1:6, legend = colnames(x), cex = .7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6FhBjuv1COK"
   },
   "source": [
    "Notice how the higher lambda results in more coefficients being shrunk towards 0, some to exactly 0.\n",
    "\n",
    "Once the estimation is complete, the R-squared values are calculated, and the coefficient and MSE plots for the two models are repeated for easier comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gmqu6o0uvPnK",
    "outputId": "305eeb9a-b9c8-4ba2-b3f1-754c2f0ecf18"
   },
   "outputs": [],
   "source": [
    "# Calculate R-squared values for the different models for comparison\n",
    "rsq <- cbind(\"R-squared\" = c(rsq_ridge_cv, rsq_lasso_cv2))\n",
    "rownames(rsq) <- c(\"ridge cross-validated\", \"lasso cross_validated\")\n",
    "print(rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yeARlSD1Zzv"
   },
   "source": [
    "Repeat the plot of solution paths for both ridge regression and the lasso for easier comparison between the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EymE2AKVvPnN",
    "outputId": "da051a92-ebee-458b-cda2-ed0d6afcb5fb"
   },
   "outputs": [],
   "source": [
    "# Repeat the plots of solution paths for comparison\n",
    "par(mfrow=c(2,2))\n",
    "\n",
    "# Ridge - top left\n",
    "plot(res, xvar = \"lambda\")\n",
    "legend(\"bottomright\", lwd = 1, col = 1:6, legend = colnames(x), cex = .7)\n",
    "\n",
    "# Ridge - top right\n",
    "plot(ridge_cv, main=\"Ridge\",cex=0.7)\n",
    "\n",
    "# Lasso - bottom left\n",
    "plot(res2, xvar = \"lambda\")\n",
    "legend(\"bottomright\", lwd = 1, col = 1:6, legend = colnames(x), cex = .7)\n",
    "\n",
    "# Lasso - bottom right\n",
    "plot(lasso_cv, main=\"LASSO\",cex=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NC6B1K9AbpFY"
   },
   "source": [
    "The top-left graph plots the solution path for ridge regression estimated coefficients. As lambda increases, the estimated coefficients shrink closer to 0. The bottom-left graph plots the solution path for the lasso estimates. As lambda increases, more coefficients shrink to exactly 0. The top-right graph plots the cross-validated error versus the log(lambda). The optimal lambda, which minimises the error, and the lambda selected by applying the one standard error rule, suggest that all 12 coefficients are non-zero. For the lasso in the bottom-right graph, the optimal lambda suggests 11 variables are selected, but lambda(1se) only selects 6 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYf4qEU_vPnQ"
   },
   "source": [
    "#### Explore further: Obtain coefficients and use for prediction\n",
    "Instead of performing separate calculations or attempting to make inferences from diagrams, the glmnet package can be used to obtain lambda at the local extrema, or the minimum mean cross-validated error, and the most regularised model. This is done using the `ridge_cv$lambda.min` function and the `ridge_cv$lambda.1se` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPqpgKKavPnT",
    "outputId": "705ae8f0-d141-4532-9fbb-c7ea71f40ac9"
   },
   "outputs": [],
   "source": [
    "# Obtain the lambda 1 standard deviation away from the local extrema\n",
    "lambda_cv2 <- ridge_cv$lambda.1se # Can use \"min\" instead of \"1se\" to obtain the minimum value\n",
    "\n",
    "# Display the model coefficients\n",
    "coef(ridge_cv, s=\"lambda.1se\")\n",
    "\n",
    "# Predict using the cross-validated model\n",
    "predict(ridge_cv, newx = x[1:10,], s = \"lambda.1se\")\n",
    "\n",
    "# Predict using the model fitted on all the data\n",
    "predict(model_cv, newx = x[1:10,], s = \"lambda.1se\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVT5L_tyvPnW",
    "outputId": "a5e1bd8d-c639-4032-9ea7-aa25be3141d1"
   },
   "outputs": [],
   "source": [
    "# Obtain the lambda 1 standard deviation away from the local extrema\n",
    "lambda_cv2 <- lasso_cv$lambda.1se # Can use \"min\" instead of \"1se\" to obtain the minimum value\n",
    "\n",
    "# Display the model coefficients\n",
    "coef(lasso_cv, s=\"lambda.1se\")\n",
    "\n",
    "# Predict using the cross-validated model\n",
    "predict(lasso_cv, newx = x[1:10,], s = \"lambda.1se\")\n",
    "\n",
    "# Predict using the model fitted on all the data\n",
    "predict(model_cv2, newx = x[1:10,], s = \"lambda.1se\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3_MD_C1vPnb"
   },
   "source": [
    "Unlike the ridge regression method, some of the variables in the data are reduced to exactly 0, which shows how the lasso reduces the number of variables. This variable selection process is one of the major advantages of the lasso."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9ZkbKZs3wAEf",
    "sNpaF_5ovPmx",
    "fd3LWr89vPnD",
    "vYf4qEU_vPnQ"
   ],
   "name": "LSE MNL M4U2 IDE Activity (Practice)_v19_GG.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
