{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wCO8sk8adF7"
   },
   "source": [
    "# LSE Machine Learning: Practical Applications\n",
    "\n",
    "## Module 5 Unit 2 IDE Activity (Assessment) | Generative models in text classification\n",
    "\n",
    "### In this IDE notebook, you are required to apply a generative model to a prepared data set in R.\n",
    "The instructions for this IDE activity are positioned as text cells before each step. As a result, you are required to read the text cells above a code cell, familiarise yourself with the required step, and then execute the step. You are encouraged to refer back to the practice IDE activity to familiarise yourself with the different steps and how they are executed in R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7o8rfTP5ML_"
   },
   "source": [
    "### Step 1: Load and install the relevant packages\n",
    "\n",
    "Load the following packages: tm, e1071, dplyr, and caret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wbt0Yjy9adF-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: NLP\n",
      "\n",
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "Loading required package: lattice\n",
      "\n",
      "Loading required package: ggplot2\n",
      "\n",
      "\n",
      "Attaching package: ‘ggplot2’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:NLP’:\n",
      "\n",
      "    annotate\n",
      "\n",
      "\n",
      "Warning message in system(\"timedatectl\", intern = TRUE):\n",
      "“running command 'timedatectl' had status 1”\n"
     ]
    }
   ],
   "source": [
    "# Load the required packages\n",
    "library(tm)\n",
    "library(e1071)\n",
    "library(dplyr)\n",
    "library(caret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5QcnCUOadGH"
   },
   "source": [
    "### Step 2: Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1xovelAmlSg"
   },
   "source": [
    "To execute the naive Bayes classifier, load the reduced newsgroups data set into R, and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PXb0MXTZadGI"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df <- read.csv(\"2_newsgroups.csv\", stringsAsFactors = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KhjKpNGnadGP",
    "outputId": "b62c61c4-493d-46ae-f3ca-4617f39d1cc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       X              text              title          \n",
       " Min.   :   0.0   Length:1192        Length:1192       \n",
       " 1st Qu.: 297.8   Class :character   Class :character  \n",
       " Median :4671.5   Mode  :character   Mode  :character  \n",
       " Mean   :2640.3                                        \n",
       " 3rd Qu.:4969.2                                        \n",
       " Max.   :5267.0                                        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmoDth_5PTQ2"
   },
   "source": [
    "### Step 3: Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtS-6QAzadGX"
   },
   "source": [
    "**Note:** In this section, you are required to perform a number of different steps to prepare the data for the model. In the practice IDE activity, you were provided with a data set that had already been prepared. In this notebook, a brief description is provided of the steps to prepare the data to create and use the naive Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FLYYv83iwog"
   },
   "source": [
    "3.1  In the following steps, you are required to build a model that attempts to predict the category of documents. As a result, convert the title to a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "p5dXR5FkadGY"
   },
   "outputs": [],
   "source": [
    "# Convert the title variable from character to factor\n",
    "df$title <- as.factor(df$title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HDh_0A_adGe"
   },
   "source": [
    "3.2. Apply the bag-of-words approach. By doing this, each word in the document is represented as a variable, while each document is represented as a vector of variables. In addition, disregard the word order and focus on the frequency of each word in the document. To do this, each document is represented as a bag of words. First, prepare a corpus of all the documents in the data frame. A corpus refers to a collection of writings. In this case, the corpus is a collection of all the documents in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UfwDNpRpadGg",
    "outputId": "8a19c329-f787-4554-dba7-2ee87e732bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<<SimpleCorpus>>\n",
       "Metadata:  corpus specific: 1, document level (indexed): 0\n",
       "Content:  documents: 1192"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SimpleCorpus>>\n",
      "Metadata:  corpus specific: 1, document level (indexed): 0\n",
      "Content:  documents: 1\n",
      "\n",
      "[1] I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "# Create and inspect the corpus\n",
    "corpus <- Corpus(VectorSource(df$text))\n",
    "corpus\n",
    "inspect(corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Or1VP3_QadGn"
   },
   "source": [
    "3.3 To prepare the data for this simplified approach, a number of different operations are applied. These include the following:\n",
    "\n",
    "-Transforming the data to lowercase\n",
    "\n",
    "-Removing punctuation\n",
    "\n",
    "-Removing numbers\n",
    "\n",
    "-Adding stopwords\n",
    "\n",
    "-Stripping out white spaces\n",
    "\n",
    "All of these actions can be completed using the tm package.\n",
    "\n",
    "Once the data has been cleaned, a built-in function of the tm package is used to create a dtm of the bag-of-word tokens. Each row of the created dtm corresponds to the documents in the collection. The columns of the dtm correspond to the terms used in the document, whereas the elements refer to the frequency of the terms appearing in these documents. \n",
    "\n",
    "Finally, by using the `inspect(dtm)` function, the newly created dtm can be analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qhs1M1NOadGo",
    "outputId": "c37785dc-e7d2-4311-9cab-a99becd23761"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in tm_map.SimpleCorpus(., content_transformer(tolower)):\n",
      "“transformation drops documents”\n",
      "Warning message in tm_map.SimpleCorpus(., removePunctuation):\n",
      "“transformation drops documents”\n",
      "Warning message in tm_map.SimpleCorpus(., removeNumbers):\n",
      "“transformation drops documents”\n",
      "Warning message in tm_map.SimpleCorpus(., removeWords, stopwords(kind = \"en\")):\n",
      "“transformation drops documents”\n",
      "Warning message in tm_map.SimpleCorpus(., stripWhitespace):\n",
      "“transformation drops documents”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<DocumentTermMatrix (documents: 11, terms: 6)>>\n",
      "Non-/sparse entries: 2/64\n",
      "Sparsity           : 97%\n",
      "Maximal term length: 9\n",
      "Weighting          : term frequency (tf)\n",
      "Sample             :\n",
      "    Terms\n",
      "Docs door doors early email engine enlighten\n",
      "  40    0     0     0     0      0         0\n",
      "  41    0     0     0     0      0         0\n",
      "  42    0     0     0     0      3         0\n",
      "  43    0     0     0     0      0         0\n",
      "  44    0     0     0     0      0         0\n",
      "  45    0     0     0     0      0         0\n",
      "  46    0     0     0     0      0         0\n",
      "  47    0     0     0     0      0         0\n",
      "  48    0     0     0     0      0         0\n",
      "  49    0     0     1     0      0         0\n"
     ]
    }
   ],
   "source": [
    "# Clean the data\n",
    "corpus.clean <- corpus %>%\n",
    "  tm_map(content_transformer(tolower)) %>% \n",
    "  tm_map(removePunctuation) %>%\n",
    "  tm_map(removeNumbers) %>%\n",
    "  tm_map(removeWords, stopwords(kind=\"en\")) %>%\n",
    "  tm_map(stripWhitespace)\n",
    "\n",
    "# Create the dtm\n",
    "dtm <- DocumentTermMatrix(corpus.clean)\n",
    "\n",
    "# Inspect the dtm\n",
    "inspect(dtm[40:50, 10:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNxHzRxxadGv"
   },
   "source": [
    "3.4 Before estimating the model, split the two sets of data into training and testing data sets. The first 70% of the data points are used to form the training data set, and the remaining 30% are used to form the test data set. Remember that the title variable is in vector form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PXOQNfGOadGw",
    "outputId": "5e69b1f3-2a30-4ec8-f339-0770dd9adc1a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>834</li><li>12350</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 834\n",
       "\\item 12350\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 834\n",
       "2. 12350\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]   834 12350"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data into training and testing data using a 70%–30% split\n",
    "set.seed(1)\n",
    "train=sample(1:nrow(df), round(nrow(df)*.7))\n",
    "df.train <- df[train,]\n",
    "df.test <- df[-train,]\n",
    "dtm.train <- dtm[train,]\n",
    "dtm.test <- dtm[-train,]\n",
    "corpus.clean.train <- corpus.clean[train]\n",
    "corpus.clean.test <- corpus.clean[-train]\n",
    "dim(dtm.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "002jkBJ_adG4"
   },
   "source": [
    "**Note:** The dtm contains 12,350 variables, but not all of them are useful for classification. For this demonstration, focus on words that appear in at least five documents, and ignore the remainder of words that do not fulfil this requirement. \n",
    "\n",
    "3.5 By using the `findFreqTerms` function, identify the frequent words. Next, this subset (freqwords) is used as a dictionary to restrict the dtm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fKjgHqtqadG5",
    "outputId": "bb496697-3919-4672-9170-156893af366f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "1970"
      ],
      "text/latex": [
       "1970"
      ],
      "text/markdown": [
       "1970"
      ],
      "text/plain": [
       "[1] 1970"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Identify the most frequent terms that appear in at least five documents\n",
    "freqwords <- findFreqTerms(dtm.train, 5)\n",
    "length((freqwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wcaHOPlJadG_",
    "outputId": "0fce76c8-7e84-4876-ca72-72eab9b2d9e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>834</li><li>1970</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 834\n",
       "\\item 1970\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 834\n",
       "2. 1970\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  834 1970"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>358</li><li>1970</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 358\n",
       "\\item 1970\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 358\n",
       "2. 1970\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  358 1970"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use freqwords to build the dtm (train and test)\n",
    "dtm.train.nb <- DocumentTermMatrix(corpus.clean.train, control=list(dictionary = freqwords))\n",
    "dim(dtm.train.nb)\n",
    "\n",
    "dtm.test.nb <- DocumentTermMatrix(corpus.clean.test, control=list(dictionary = freqwords))\n",
    "dim(dtm.test.nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Jw40JOdadHF"
   },
   "source": [
    "3.6 In the next step, the term frequencies are replaced with boolean presence or boolean absence variables. This means that the parameter used to predict the class variable is either yes or no. This type of naive Bayes algorithm, which is a multinomial naive Bayes algorithm, is known as binarised (boolean variable) naive Bayes. Note that the specifics of this algorithm fall outside the scope of this course.\n",
    "\n",
    "Replacing the term frequencies with boolean variables (i.e. yes or no variables) is often applied to sentiment analysis, but is used in this example as well. The logic that underpins this step is based on the assumption that word occurrence is more important than word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gLK5lXgAadHH"
   },
   "outputs": [],
   "source": [
    "# Convert counts\n",
    "convert_count <- function(x) {\n",
    "  y <- ifelse(x > 0, 1,0)\n",
    "  y <- factor(y, levels=c(0,1), labels=c(\"absent\", \"present\"))\n",
    "\n",
    "  y\n",
    "}\n",
    "trainNB <- apply(dtm.train.nb, 2, convert_count)\n",
    "testNB <- apply(dtm.test.nb, 2, convert_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1omB-kGmIJ_"
   },
   "source": [
    "### Step 4: Estimate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aw7tb6AqadHP"
   },
   "source": [
    "Now that the data sets have been split into training and testing sets, estimate the model using the `classifier` function to perform the naive Bayes classification. The parameters used are the dtm object, ```trainNB```, and the class object, ```df.train$title```. In this example, you can also set the laplace parameter equal to 1. The laplace parameter falls outside the scope of this course.\n",
    "\n",
    "**Hint:**\n",
    "> classifier <- naiveBayes(dtm, class, laplace = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "id": "DkibXbGWadHQ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fa5c66ae70cabf2ba706497316405c9",
     "grade": true,
     "grade_id": "cell-5f0e575475bde0e5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": "classifier <- naiveBayes(trainNB, df.train$title, laplace = 1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al7V3v8wmRIY"
   },
   "source": [
    "### Step 5: Make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpK_NIYaadHX"
   },
   "source": [
    "Use the `predict` function to predict the classification on the training data set. Use the `predict` function with your classifier and the `newdata` parameter set to `testNB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "id": "ouHsu1cbadHY",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d42b83bfdf079c215ed441724b08a35a",
     "grade": true,
     "grade_id": "cell-d88d7cf90e343993",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": "testPreds<- predict(classifier, newdata=testNB)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XV-gSNlmV9f"
   },
   "source": [
    "### Step 6: Test model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYVeDu4madHe"
   },
   "source": [
    "Test the model's performance by creating a confusion matrix using the `confusionMatrix` function. The prediction argument consists of the predictions that were created in the previous step, and it originates from the training class vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "id": "uJiImc-wadHf",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f781fbbc59c780b9afbc74f48c7bef4",
     "grade": true,
     "grade_id": "cell-c3820234859c787f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "f1556cce-0303-4d12-9d3e-823ba1c9906b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "                 Reference\n",
       "Prediction        rec.autos rec.motorcycles\n",
       "  rec.autos             156              19\n",
       "  rec.motorcycles        32             151\n",
       "                                         \n",
       "               Accuracy : 0.8575         \n",
       "                 95% CI : (0.817, 0.8921)\n",
       "    No Information Rate : 0.5251         \n",
       "    P-Value [Acc > NIR] : < 2e-16        \n",
       "                                         \n",
       "                  Kappa : 0.7154         \n",
       "                                         \n",
       " Mcnemar's Test P-Value : 0.09289        \n",
       "                                         \n",
       "            Sensitivity : 0.8298         \n",
       "            Specificity : 0.8882         \n",
       "         Pos Pred Value : 0.8914         \n",
       "         Neg Pred Value : 0.8251         \n",
       "             Prevalence : 0.5251         \n",
       "         Detection Rate : 0.4358         \n",
       "   Detection Prevalence : 0.4888         \n",
       "      Balanced Accuracy : 0.8590         \n",
       "                                         \n",
       "       'Positive' Class : rec.autos      \n",
       "                                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the confusion matrix\n",
    "\n",
    "#example <- confusionMatrix(data=predicted_value, reference = expected_value)\n",
    "ConM<-confusionMatrix(data=testPreds,reference=df.test$title)\n",
    "ConM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvXvxLS2adHk"
   },
   "source": [
    "The accuracy of the model indicates the percentage of times the model predicts the class or title correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOQWcMaFo7TC"
   },
   "source": [
    "**Note:** Remember to submit this IDE notebook after completion and navigate to the activity submission to submit the written component of this assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQT-IYilR6c-"
   },
   "source": [
    "# Bibliography:\n",
    "Katti, R. 2016. _Naive Bayes classification for sentiment analysis of movie reviews._ Available: https://rpubs.com/cen0te/naivebayes-sentimentpolarity [2020, April 7]."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSE MNL M5U2 IDE Activity (assessment)_v11_GG.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "LSE-MNL-R",
   "language": "R",
   "name": "lse-mnl-r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
